#version=RHEL7
# System authorization information
auth --enableshadow --passalgo=sha512

# Perform kickstart installation in text mode
text

# Use CDROM installation media
cdrom
# Run the Setup Agent on first boot
firstboot --enable
ignoredisk --only-use=sda,sdb
# Keyboard layouts
keyboard --vckeymap=de --xlayouts='de (nodeadkeys)'
# System language
lang en_US.UTF-8

# Network information
network  --hostname=virthost01.<your-domain>
#virthost02: network  --hostname=virthost02.<your-domain>
#virthost03: network  --hostname=virthost03.<your-domain>

# admin network:
## HOST1: ##
network  --bootproto=static --device=enp2s0 --onboot=on --ip=172.21.0.101 --netmask=255.255.255.0 --gateway=172.21.0.1 --nameserver=172.21.0.1 --activate
## HOST2: ##
#virthost02: network  --bootproto=static --device=00:23:7d:4a:6f:ca --onboot=on --ip=172.21.0.102 --netmask=255.255.255.0 --gateway=172.21.0.1 --nameserver=172.21.0.1 --activate
## HOST3: ##
#virthost03: network  --bootproto=static --device=00:1d:72:88:26:ae --onboot=on --ip=172.21.0.103 --netmask=255.255.255.0 --gateway=172.21.0.1 --nameserver=172.21.0.1 --activate

# public network:
## HOST1: ##
network  --bootproto=dhcp --device=enp3s0 --onboot=on --ipv6=auto --activate
## HOST2: ##
#virthost02: network  --bootproto=dhcp --device=00:50:b6:49:a6:14 --onboot=on --ipv6=auto --activate
## HOST3: ##
#virthost03: network  --bootproto=dhcp --device=00:50:b6:49:a6:14 --onboot=on --ipv6=auto --activate

# Root password
rootpw --iscrypted $6$
# System services
services --enabled="chronyd"
# System timezone
timezone Europe/Berlin --isUtc --ntpservers=0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org,3.pool.ntp.org
# System bootloader configuration
bootloader --location=mbr --boot-drive=sda
# Partition clearing information
# MIRRORED HD INSTALL:
clearpart --all --initlabel --drives=sda,sdb
# SINGLE HD INSTALL: clearpart --all --initlabel --drives=sda
# Disk partitioning information
part biosboot --fstype="biosboot" --ondisk=sda --asprimary --size=1
# MIRRORED HD INSTALL:
part /biosboot_sdb --fstype="biosboot" --ondisk=sdb --asprimary --size=1
part raid.11 --fstype="mdmember" --ondisk=sda --asprimary --size=500
part raid.12 --fstype="mdmember" --ondisk=sdb --asprimary --size=500
part raid.21 --fstype="mdmember" --ondisk=sda --asprimary --size=100000 --grow
part raid.22 --fstype="mdmember" --ondisk=sdb --asprimary --size=100000 --grow
raid /boot --device=md0 --fstype="xfs" --level=1 raid.11 raid.12
raid pv.01 --device=md1 --encrypted --passphrase=1234567890 --level=1 raid.21 raid.22
# SINGLE HD INSTALL: part /boot --ondisk=sda --asprimary --size=500 --fstype="xfs"
# SINGLE HD INSTALL: part pv.01 --ondisk=sda --asprimary --size=100000 --grow --encrypted --passphrase=1234567890
volgroup vg_virthost01 --pesize=4096 pv.01
logvol /  --fstype="xfs" --size=20000 --name=root --vgname=vg_virthost01
logvol swap  --fstype="swap" --size=16136 --name=swap --vgname=vg_virthost01
logvol /var/log  --fstype="xfs" --size=1956 --name=var_log --vgname=vg_virthost01
logvol /gluster  --fstype="xfs" --size=50000 --name=gluster --vgname=vg_virthost01


%packages
@base
@core
@network-file-system-client
@remote-system-management
@virtualization-hypervisor
@virtualization-platform
@virtualization-tools
chrony
screen
nfs-utils
system-storage-manager
ctdb

%end

# self-hosted engine setup with gluster storage taken from
# http://community.redhat.com/blog/2014/10/up-and-running-with-ovirt-3-5/ and
# http://community.redhat.com/blog/2014/11/up-and-running-with-ovirt-3-5-part-two/
%post
# multipathd throws ugly errors, thus we blacklist the harddisks
cat >> /etc/multipath.conf <<EOF

blacklist {
       devnode "^sd[a-b]"
}
EOF

# make hostnames locally resolveable
cat >>/etc/hosts <<EOF
#172.21.0.99	virtstorage.<your-domain> virtstorage
# 00:16:3e:01:01:01
172.21.0.100	virtmanager.<your-domain> virtmanager
# 00:60:6e:39:01:2f
172.21.0.101	virthost01.<your-domain> virthost01
# possible next servers
172.21.0.102	virthost02.<your-domain> virthost02
172.21.0.103	virthost03.<your-domain> virthost03
172.21.0.101	virtstorage.<your-domain> virtstorage
EOF

# configure nested virtualization for the host
cat >/etc/modprobe.d/kvm-nested.conf <<EOF
# create new
options kvm_intel nested=1
EOF

# update installation
yum -y update

# disable firewall and network-manager
systemctl disable firewalld && systemctl stop firewalld
systemctl enable iptables
systemctl disable NetworkManager && systemctl stop NetworkManager

# install ovirt packages
yum localinstall -y http://resources.ovirt.org/pub/yum-repo/ovirt-release35.rpm
yum install -y ovirt-hosted-engine-setup glusterfs-server vdsm-gluster vdsm-hook-nestedvt vdsm-hook-macspoof vdsm-hook-hostusb vdsm-hook-isolatedprivatevlan ovirt-engine-sdk-python

# adding ovirt post-install instructions as bash script to /root/
cat >/root/step_1_on_virthost01_hosted-engine-deploy.sh <<EOF
#!/bin/sh
# see whether blacklisting from kickstart times still exist and if no, add it to multipath.conf again:
grep "^blacklist" /etc/multipath.conf >/dev/null || {
 cat >> /etc/multipath.conf <<IEOF

blacklist {
       devnode "^sd[a-b]"
}
IEOF
}
#ssm add -p gluster \$YOUR_DEVICE_NAME
#ssm create -p gluster --fstype xfs -n gluster
#mkdir /gluster
#UUID_GLUSTER=\$(blkid /dev/gluster/gluster)
#echo "UUID=\$UUID_GLUSTER /gluster xfs defaults 0 0" >> /etc/fstab
#mount -a
sed -i "s/Lock=True/Lock=True\nLock=False/" /etc/nfsmount.conf
gluster volume status | grep "volume: engine" || {
 mkdir -p /gluster/{engine,images,iso,export,meta}/brick && mkdir /mnt/lock
 systemctl start glusterd && systemctl enable glusterd
 systemctl stop glusterd && rm -f /var/run/glusterd.socket && systemctl start glusterd
 for I in \$(ls /gluster); do
  gluster volume create \$I \$HOSTNAME:/gluster/\$I/brick
  gluster volume set \$I group virt
  gluster volume set \$I storage.owner-uid 36 && gluster volume set \$I storage.owner-gid 36
  gluster volume start \$I
 done
 gluster volume status
}
grep "/mn/lock" /proc/mounts || mount -t glusterfs localhost:/meta /mnt/lock
cat >/mnt/lock/ctdb <<IEOF
CTDB_PUBLIC_ADDRESSES=/mnt/lock/public_addresses
CTDB_NODES=/etc/ctdb/nodes
# Only when using Samba. Unnecessary for NFS.
CTDB_MANAGES_SAMBA=no
# some tunables
CTDB_SET_DeterministicIPs=1
CTDB_SET_RecoveryBanPeriod=120
CTDB_SET_KeepaliveInterval=5
CTDB_SET_KeepaliveLimit=5
CTDB_SET_MonitorInterval=15
CTDB_RECOVERY_LOCK=/mnt/lock/reclock
IEOF
cat >/mnt/lock/nodes <<IEOF
172.21.0.101
IEOF
cat >/mnt/lock/public_addresses <<IEOF
172.21.0.99/24 ovirtmgmt
IEOF
# download CentOS ISO from local share or USB stick; alternatively get it from upstream
ls /home/tmp/CentOS*.iso 2>/dev/null 2>&1 && {
 ln -s \$(ls -1 /home/tmp/CentOS*.iso | head -n1) /home/tmp/hosted-engine-install.iso
 chown -R 36:36 /home/tmp
}
echo "As an installation media for the hosted engine, the USB stick this host was installed with, is sufficient; it has been copied to /home/tmp/ already:"
ls -l /home/tmp/
echo "If you want to use a different media, just put it under /home/tmp now and link /home/tmp/hosted-engine-install.iso to it."
#echo;echo "Partition table:"
#grep sd /proc/partitions
#mkdir -p /home/tmp
#ls -l /home/tmp/
#echo;echo "Download CentOS 7 ISO, copy it from USB stick or put it manually to /home/tmp? [D/C/M] (Download/Copy/Manual) "; read ANSWER
#test "\$ANSWER" = "D" && {
 #mkdir -p /home/tmp && cd /home/tmp && test -e CentOS-6.6-x86_64-minimal.iso || curl -k -o CentOS-6.6-x86_64-minimal.iso "https://192.168.178.1:483/nas/filelink.lua?id=edf40297e346481e"
# test -e /home/tmp/CentOS-7-x86_64-Minimal-1503-01.iso || curl -k -o /home/tmp/CentOS-7-x86_64-Minimal-1503-01.iso "https://192.168.178.1:483/nas/filelink.lua?id=757a27d3e296bd45"
# cd
 #cd /home/tmp/ && curl -O http://mirrors.kernel.org/centos/6.6/isos/x86_64/CentOS-6.6-x86_64-minimal.iso
 #cd /home/tmp/ && curl -O http://mirrors.kernel.org/centos/7.1.1503/isos/x86_64/CentOS-7-x86_64-Minimal-1503-01.iso
#}
#test "\$ANSWER" = "C" && dd if=/dev/sdc1 of=/home/tmp/CentOS-7-x86_64-Everything-1503-01.iso bs=1M
#test "\$ANSWER" = "M" && touch /home/tmp/CentOS-7-x86_64-Everything-1503-01.iso
#ls -l /home/tmp/
echo;echo -n "Continue? <CTRL-C> to cancel."; read ANSWER
test -h /home/tmp/hosted-engine-install.iso || ln -s \$(ls -1 /home/tmp/CentOS*.iso | head -n1) /home/tmp/hosted-engine-install.iso
chown -R 36:36 /home/tmp
ls -l /home/tmp/
# start hosted engine deployment; use --config-append=<config-file> to provide an answer file
echo;echo -n "Run \"hosted-engine --deploy --config-append=/root/step_1_on_virthost01_hosted-engine-deploy.answers\" ? <CTRL-C> to cancel."; read ANSWER
echo;echo "*** IMPORTANT: Make sure to set the network configuration of the virtual machine to static IP 172.21.0.100/24, make it activate on boot and set the hostname to virtmanager.<your-domain>! ***"
sed -i "s/^#%packages/%packages/g;s/^#%end/%end/g;s/^rootpw.*/\$(grep '^rootpw --iscrypted' /root/anaconda-ks.cfg | sed "s/\//\\\\\\\//g")/g" /root/virtmanager-ks.cfg
# make kickstart file for virtmanager available via nfs share:
mv  /root/virtmanager-ks.cfg /mnt/lock/
echo "Alternatively, install the virtual machine automatically using kickstart by adding \"ksdevice=eth0 ip=172.21.0.100 netmask=255.255.255.0 ks=nfs:172.21.0.101:/meta/virtmanager-ks.cfg\" to the boot kernel command line after having executed /root/iptables-open-nfs-ports.sh on virthost01."
echo;echo
hosted-engine --deploy --config-append=/root/step_1_on_virthost01_hosted-engine-deploy.answers
echo;echo -n "If the previous command has finished successfully, continue setup now? <CTRL-C> to cancel."; read ANSWER
sed -i "s/172.21.0.101\tvirtstorage.*//;s/^#172.21.0.99/172.21.0.99/" /etc/hosts
cp -v /root/iptables.ovirt_gluster_ctdb /etc/sysconfig/iptables && systemctl reload iptables
# CTDB doesn't work with SELinux enabled if config files are symlinked to /mnt/lock:
setenforce 0; sed -i "s/SELINUX=enforcing/SELINUX=permissive/g" /etc/selinux/config
mv /etc/sysconfig/ctdb /etc/sysconfig/ctdb.orig && ln -s /mnt/lock/ctdb /etc/sysconfig/ctdb && ln -s /mnt/lock/nodes /etc/ctdb/nodes && ln -s /mnt/lock/public_addresses /etc/ctdb/public_addresses
cat >/etc/systemd/system/ctdb.service <<IEOF
[Unit]
Description=CTDB
After=mnt-lock.mount
Requires=mnt-lock.mount
Requires=glusterd.service

[Service]
Type=forking
LimitCORE=infinity
PIDFile=/run/ctdb/ctdbd.pid
ExecStart=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid start
ExecStop=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid stop
KillMode=control-group
Restart=on-failure

[Install]
WantedBy=multi-user.target
IEOF
systemctl enable ctdb.service

cat >/etc/systemd/system/mnt-lock.mount <<IEOF
[Unit]
Description=ctdb meta volume
Requires=glusterd.service
Before=ctdb.service

[Mount]
What=localhost:meta
Where=/mnt/lock
Type=glusterfs
Options=defaults,_netdev

[Install]
WantedBy=multi-user.target
IEOF
systemctl start mnt-lock.mount && systemctl enable mnt-lock.mount
systemctl start ctdb && systemctl enable ctdb
echo "Installation finished; run \"step_2_on_virthost01_hosted-engine-cleanup.sh\"? <CTRL-C> to cancel."; read ANSWER
./step_2_on_virthost01_hosted-engine-cleanup.sh
EOF
chmod u+x /root/step_1_on_virthost01_hosted-engine-deploy.sh

cat >/root/step_1_on_virthost01_hosted-engine-deploy.answers <<EOF
[environment:default]
OVEHOSTED_NETWORK/bridgeIf=str:enp2s0
OVEHOSTED_CORE/deployProceed=bool:True
OVEHOSTED_CORE/confirmSettings=bool:True
OVEHOSTED_NETWORK/fqdn=str:virtmanager.<your-domain>
OVEHOSTED_NETWORK/bridgeName=str:ovirtmgmt
OVEHOSTED_NETWORK/firewallManager=str:iptables
OVEHOSTED_NETWORK/gateway=str:virtstorage.<your-domain>
OVEHOSTED_ENGINE/clusterName=str:Default
OVEHOSTED_ENGINE/appHostName=str:VirtHost01
OVEHOSTED_STORAGE/storageDomainName=str:hosted_storage
OVEHOSTED_STORAGE/hostID=int:1
OVEHOSTED_STORAGE/storageDatacenterName=str:hosted_datacenter
OVEHOSTED_STORAGE/domainType=str:nfs3
OVEHOSTED_STORAGE/imgAlias=str:hosted_engine
OVEHOSTED_STORAGE/imgSizeGB=str:25
OVEHOSTED_STORAGE/storageDomainConnection=str:virtstorage.<your-domain>:/engine
OVEHOSTED_VDSM/consoleType=str:vnc
OVEHOSTED_VM/vmMemSizeMB=str:4096
OVEHOSTED_VM/vmMACAddr=str:00:16:3e:01:01:01
OVEHOSTED_VM/emulatedMachine=str:pc
OVEHOSTED_VM/vmBoot=str:cdrom
OVEHOSTED_VM/vmVCpus=str:2
#OVEHOSTED_VM/vmCDRom=str:/home/tmp/CentOS-6.6-x86_64-minimal.iso
OVEHOSTED_VM/vmCDRom=str:/home/tmp/hosted-engine-install.iso
OVEHOSTED_VDSM/cpu=str:model_Penryn
OVEHOSTED_NOTIF/smtpPort=str:25
OVEHOSTED_NOTIF/smtpServer=str:localhost
OVEHOSTED_NOTIF/sourceEmail=str:root@localhost
OVEHOSTED_NOTIF/destEmail=str:root@localhost
EOF

cat >/root/step_1.1_on_virthost01_hosted-engine-configure.sh <<EOF
#!/bin/sh
echo -n "Waiting for virtmanager to come online."
while :; do
 ping -q -c2 -W10 virtmanager >/dev/null 2>&1 && break
 echo -n "."
done
echo
ip add sh ovirtmgmt | grep "172.21.0.99" || ip addr add 172.21.0.99/24 dev ovirtmgmt
echo "you can use any password in the following dialog, it is only needed for the following virsh commands which ask several times for this (username is \"ovirt\" each time):"
echo "- Creating sasl password:"
saslpasswd2 -a libvirt ovirt
echo "- creating bridge to public network to let virtmanager access the internet:"
brctl show | grep publicbridge || virsh iface-bridge enp3s0 publicbridge
echo "- attaching virtmanager to the public bridge:"
brctl show | grep -A1 publicbridge | grep vnet || virsh attach-interface HostedEngine bridge publicbridge
brctl show | grep -A1 publicbridge | grep vnet || echo "*** Network to virtmanager could not be set up! ***"
echo "- copy files step_1.2* and virthost0[2,3]-ks.cfg over to virtmanager:"
while :; do
 scp /root/step_1.2* /root/virthost0[2,3]-ks.cfg root@virtmanager:/root/ && break
 echo -n "Make sure virtmanager is reachable. Press any key to try again or continue setup anyway with \"c\": "
 read ANSW
 test "\$ANSWER" = "c" && break
done
echo "Installation files copied to virtmanager successfully."
echo "- logging into virtmanager to configure public network and fetch updates including ovirt packages:"
while :; do 
 ssh root@virtmanager 'ping -q -c2 -W10 ntp.org >/dev/null 2>&1 || { ip add sh | grep '192.168.178' || ip addr add 192.168.178.10/24 dev \$(ip li sh | grep -v "eth0\|lo\|link" | cut -d" " -f2 | cut -d":" -f1 | head -n1); ip route sh | grep "^default via 192.168.178.1" || ip route add default via 192.168.178.1 dev \$(ip li sh | grep -v "eth0\|lo\|link" | cut -d" " -f2 | cut -d":" -f1 | head -n1); grep "nameserver 192.168.178.1" /etc/resolv.conf || echo "nameserver 192.168.178.1" >>/etc/resolv.conf; }; grep "^172.21.0.99" /etc/hosts >/dev/null 2>&1 || { echo "172.21.0.99	virtstorage.<your-domain> virtstorage" >>/etc/hosts; echo "172.21.0.100	virtmanager.<your-domain> virtmanager" >>/etc/hosts; echo "172.21.0.101	virthost01.<your-domain> virthost01" >>/etc/hosts; echo "172.21.0.102	virthost02.<your-domain> virthost02" >>/etc/hosts; } ; rpm -q ovirt-engine >/dev/null 2>&1 || { yum -y update >/dev/null; yum localinstall -y http://resources.ovirt.org/pub/yum-repo/ovirt-release35.rpm >/dev/null; yum install -y ovirt-engine screen ovirt-guest-agent ovirt-engine-dwh ovirt-engine-reports ovirt-engine-sdk-python >/dev/null; }' && break
 echo -n "Make sure virtmanager is reachable. Press any key to try again or continue setup anyway with \"c\": "
 read ANSW; test "\$ANSWER" = "c" && break
done
echo "  -> virtmanager should be configured and updated now."
echo "Shutting down the VM \"virtmanager\" now; choose \"[2] Power off and restart the VM\" when the VM has shut down and press <Enter>, when VM is up and running again:";echo;echo
#ps aux|grep "qemu-kvm.*HostedEngine"|grep -v grep
virsh shutdown HostedEngine
echo;echo -n "Waiting for VM to shut down."
while [ -n "\$(ps aux|grep "qemu-kvm.*HostedEngine" | grep -v grep)" ]; do echo -n "."; sleep 10; done
echo; echo "VM shut down; after VM is up again, press any key to continue."; read ANSWER; echo
echo -n "Waiting for virtmanager to come online."
while :; do
 ping -q -c2 -W10 virtmanager >/dev/null 2>&1 && break
 echo -n "."
done
echo
echo "- re-creating public bridge:"
brctl show | grep publicbridge || virsh iface-bridge enp3s0 publicbridge
echo "- re-attaching virtmanager to public bridge:"
brctl show | grep -A1 publicbridge | grep vnet || virsh attach-interface HostedEngine bridge publicbridge
brctl show | grep -A1 publicbridge | grep vnet || echo "*** Network to virtmanager could not be set up! ***"
echo "- re-configuring virtmanager's public network:"
while :; do 
 ssh root@virtmanager 'ping -q -c2 -W10 ntp.org >/dev/null 2>&1 || { ip add sh | grep '192.168.178' || ip addr add 192.168.178.10/24 dev \$(ip li sh | grep -v "eth0\|lo\|link" | cut -d" " -f2 | cut -d":" -f1 | head -n1); ip route sh | grep "^default via 192.168.178.1" || ip route add default via 192.168.178.1 dev \$(ip li sh | grep -v "eth0\|lo\|link" | cut -d" " -f2 | cut -d":" -f1 | head -n1); grep "nameserver 192.168.178.1" /etc/resolv.conf || echo "nameserver 192.168.178.1" >>/etc/resolv.conf; }; grep "^172.21.0.99" /etc/hosts >/dev/null 2>&1 || { echo "172.21.0.99	virtstorage.<your-domain> virtstorage" >>/etc/hosts; echo "172.21.0.100	virtmanager.<your-domain> virtmanager" >>/etc/hosts; echo "172.21.0.101	virthost01.<your-domain> virthost01" >>/etc/hosts; echo "172.21.0.102	virthost02.<your-domain> virthost02" >>/etc/hosts; }' && break
 echo -n "Make sure virtmanager is reachable. Press any key to try again or continue setup anyway with \"c\": "
 read ANSW; test "\$ANSWER" = "c" && break
done
echo;echo "Running \"step_1.2.2_on_virtmanager_hosted-engine-setup.sh\" on virtmanager now."
while :; do
 ssh root@virtmanager '/root/step_1.2.2_on_virtmanager_hosted-engine-setup.sh' && break
 echo -n "Make sure virtmanager is reachable. Press any key to try again or continue setup anyway with \"c\": "
 read ANSW; test "\$ANSWER" = "c" && break
done
EOF
chmod u+x /root/step_1.1_on_virthost01_hosted-engine-configure.sh

cat >/root/step_1.2.2_on_virtmanager_hosted-engine-setup.sh <<EOF
#!/bin/sh
echo "Running \"engine-setup --config=/root/step_1.2.2_on_virtmanager_hosted-engine-setup.answers --jboss-home=\$(ls -d /usr/share/*jboss-as)\" now..."
engine-setup --config=/root/step_1.2.2_on_virtmanager_hosted-engine-setup.answers --jboss-home=\$(ls -d /usr/share/*jboss-as)
echo; echo "Enabling nested virtualization by activating macspoof hook..."
engine-config -s "UserDefinedVMProperties=macspoof=(true|false)"
echo "Enabling attaching USB devices to VMs by activating hostusb hook..."
engine-config -s "UserDefinedVMProperties=hostusb=[\w:&]+"
echo
mkdir -p /var/www/html/ks/ && mv -v /root/virthost0[2,3]-ks.cfg /var/www/html/ks/
echo "Please modify /var/www/html/ks/virthost0[2,3]-ks.cfg to your needs before using them!"
echo "Now head back to the hosted-engine --deploy terminal and enter \"(1) Continue setup\" to complete installation"
EOF
chmod u+x /root/step_1.2.2_on_virtmanager_hosted-engine-setup.sh

cat >/root/step_1.2.2_on_virtmanager_hosted-engine-setup.answers <<EOF
# action=setup
[environment:default]
OVESETUP_DIALOG/confirmSettings=bool:True
OVESETUP_CONFIG/applicationMode=str:both
#OVESETUP_CONFIG/remoteEngineSetupStyle=none:None
#OVESETUP_CONFIG/adminPassword=str:<pwd>
#OVESETUP_CONFIG/storageIsLocal=bool:False
#OVESETUP_CONFIG/firewallManager=str:iptables (RHEL 6.x only)
OVESETUP_CONFIG/firewallManager=str:firewalld
#OVESETUP_CONFIG/remoteEngineHostRootPassword=none:None
OVESETUP_CONFIG/updateFirewall=bool:True
#OVESETUP_CONFIG/remoteEngineHostSshPort=none:None
OVESETUP_CONFIG/fqdn=str:virtmanager.<your-domain>
#OVESETUP_CONFIG/storageType=none:None
#OSETUP_RPMDISTRO/requireRollback=none:None
#OSETUP_RPMDISTRO/enableUpgrade=none:None
OVESETUP_DB/database=str:engine
#OVESETUP_DB/fixDbViolations=none:None
OVESETUP_DB/secured=bool:False
OVESETUP_DB/host=str:localhost
OVESETUP_DB/user=str:engine
OVESETUP_DB/securedHostValidation=bool:False
OVESETUP_DB/port=int:5432
OVESETUP_ENGINE_CORE/enable=bool:True
#OVESETUP_CORE/engineStop=none:None
OVESETUP_SYSTEM/memCheckEnabled=bool:True
OVESETUP_SYSTEM/nfsConfigEnabled=bool:False
OVESETUP_PKI/organization=str:<your-domain>
#OVESETUP_CONFIG/isoDomainMountPoint=str:/var/lib/exports/iso
#OVESETUP_CONFIG/isoDomainName=str:ISO_DOMAIN
#OVESETUP_CONFIG/isoDomainACL=str:*.<your-domain>(rw)
#OVESETUP_AIO/configure=none:None
#OVESETUP_AIO/storageDomainName=none:None
#OVESETUP_AIO/storageDomainDir=none:None
OVESETUP_PROVISIONING/postgresProvisioningEnabled=bool:True
OVESETUP_APACHE/configureRootRedirection=bool:True
OVESETUP_APACHE/configureSsl=bool:True
OVESETUP_DWH_CORE/enable=bool:True
#OVESETUP_DWH_DB/database=str:ovirt_engine_history
#OVESETUP_DWH_DB/secured=bool:False
OVESETUP_DWH_DB/host=str:localhost
#OVESETUP_DWH_DB/disconnectExistingDwh=none:None
#OVESETUP_DWH_DB/restoreBackupLate=bool:True
#OVESETUP_DWH_DB/user=str:ovirt_engine_history
#OVESETUP_DWH_DB/securedHostValidation=bool:False
#OVESETUP_DWH_DB/performBackup=none:None
#OVESETUP_DWH_DB/password=str:T8hiFutjVvrnD9NSQCx14o
#OVESETUP_DWH_DB/port=int:5432
OVESETUP_DWH_PROVISIONING/postgresProvisioningEnabled=bool:True
OVESETUP_REPORTS_CORE/enable=bool:True
#OVESETUP_REPORTS_CONFIG/heapMin=str:1024M
#OVESETUP_REPORTS_CONFIG/adminPassword=str:XXXXXXXXXXXX
#OVESETUP_REPORTS_CONFIG/heapMax=str:1024M
#OVESETUP_REPORTS_DB/database=str:ovirt_engine_reports
#OVESETUP_REPORTS_DB/secured=bool:False
OVESETUP_REPORTS_DB/host=str:localhost
#OVESETUP_REPORTS_DB/user=str:ovirt_engine_reports
#OVESETUP_REPORTS_DB/securedHostValidation=bool:False
#OVESETUP_REPORTS_DB/password=str:GEJmzLFexE6gTFEPGc5M1I
#OVESETUP_REPORTS_DB/port=int:5432
OVESETUP_REPORTS_PROVISIONING/postgresProvisioningEnabled=bool:True
OVESETUP_CONFIG/websocketProxyConfig=bool:True
OVESETUP_ENGINE_CONFIG/fqdn=str:virtmanager.<your-domain>
EOF

cat >/root/step_2_on_virthost01_hosted-engine-cleanup.sh <<EOF
#!/bin/sh
ip add sh ovirtmgmt | grep "172.21.0.99" || ip addr add 172.21.0.99/24 dev ovirtmgmt
virsh iface-unbridge publicbridge
ifdown enp3s0; sed -i "s/ONBOOT=.*/ONBOOT=no/" /etc/sysconfig/network-scripts/ifcfg-enp3s0
rm /etc/sysconfig/network-scripts/ifcfg-enp*
rm /etc/sysconfig/network-scripts/ifcfg-'[DMZ,Public]'
echo "" > /etc/resolv.conf
# see whether blacklisting from kickstart times still exist and if no, add it to multipath.conf again:
grep "^blacklist" /etc/multipath.conf || {
 cat >> /etc/multipath.conf <<IEOF

blacklist {
       devnode "^sd[a-b]"
}
IEOF
}
echo
echo "- you can now attach virtstorage.<your-domain>:/images as a Gluster storage domain to the datacenter"
echo "- you can now attach virtstorage.<your-domain>:/iso as an NFS ISO domain to the datacenter"
echo "- you can now move \$(ls -1 /home/tmp/CentOS*iso) to the ISO Domain by using:"
echo "  # mv \$(ls -1 /home/tmp/CentOS*iso) \$(echo /gluster/iso/brick/*/images/1*/)"
echo
echo "- you can now attach virtstorage.<your-domain>:/export as an NFS Export domain to the datacenter"
echo
echo "- you can now create a \"Public\" network in the datacenter and attach enp3s0 to it on virthost01"
echo
echo "- you can now create a \"DMZ\" network in the datacenter and attach enp4s1 to it on virthost01"
echo
echo "- you can now change the harddisk encryption password to something better than 1234567890 by running:"
echo "  # cryptsetup status /dev/mapper/luks-* | grep device"
echo "  # cryptsetup luksChangeKey /dev/md1 (or whatever device appeared above)
echo"
echo "- reboot this host to activate nested virtualization and make sure everything comes up as expected:"
echo "  # hosted-engine --set-maintenance --mode=global"
echo "  # hosted-engine --vm-shutdown"
echo "  --- wait a few minutes ---"
echo "  # ps aux | grep qemu | grep -v grep || reboot"
echo "  -> don't forget to leave maintenance mode:"
echo "  # hosted-engine --set-maintenance --mode=none"
echo;echo -n "If you want to proceed rebooting this host, press any key, otherwise press <CTRL-C> to quit setup. "; read ANSWER
/root/hosted-engine-shutdown.sh
ps aux | grep qemu | grep -v grep || systemctl reboot
echo;echo "Cancelled reboot as there are still virtual machines running..."
EOF
chmod u+x /root/step_2_on_virthost01_hosted-engine-cleanup.sh

cat >/root/hosted-engine-shutdown.sh <<EOF
#!/bin/sh
hosted-engine --set-maintenance --mode=global && hosted-engine --vm-shutdown
while :; do
 hosted-engine --vm-status | grep "Engine status" | grep "down" | grep -v "powering" && break
 hosted-engine --vm-status | grep "Engine status"
 sleep 5
done
hosted-engine --vm-status
EOF
chmod u+x /root/hosted-engine-shutdown.sh

cat >/root/hosted-engine-start.sh <<EOF
#!/bin/sh
echo "Starting hosted engine..."
hosted-engine --set-maintenance --mode=none && hosted-engine --vm-start
while :; do
 hosted-engine --vm-status | grep "Engine status" | grep "up" | grep -v "powering" && break
 hosted-engine --vm-status | grep "Engine status"
 sleep 5
done
hosted-engine --vm-status
EOF
chmod u+x /root/hosted-engine-start.sh

cat >/root/run-ovirt-command.sh <<EOF
#!/bin/sh
echo "Running: ovirt-shell -c -l "https://virtmanager:443/api" -I -u "admin@internal" -E "\$*""
ovirt-shell -c -l "https://virtmanager:443/api" -I -u "admin@internal" -E "\$*"
EOF
chmod u+x /root/run-ovirt-command.sh

cat >/root/kickstart-centos7-vm.py <<EOF
#!/usr/bin/env python
from ovirtsdk.api import API
from ovirtsdk.xml import params

URL      = "https://virtmanager.<your-domain>/api"
USERNAME = "admin@internal"
PASSWORD = "password"
CA_FILE  = "/etc/pki/ovirt-engine/ca.pem"

api = API(url=URL, username=USERNAME, password=PASSWORD, ca_file=CA_FILE)

# ---
VM_NAME = "my_vm"
CLUSTER_NAME = "default"
SOCKETS = 2
CORES = 2
GB = 1024**3

cpu_params = params.CPU(topology=params.CpuTopology(sockets=SOCKETS,
                                                    cores=CORES))
api.vms.add(params.VM(name=VM_NAME,
                      cluster=api.clusters.get(CLUSTER_NAME),
                      template=api.templates.get("Blank"),
                      cpu=cpu_params,
                      memory=2*GB,
                      display=params.Display(type_="SPICE")))
# ---
import time

def wait_vm_state(vm_name, state):
    while api.vms.get(vm_name).status.state != state:
        time.sleep(1)

wait_vm_state(VM_NAME, "down")

# ---
STG_DOMAIN = "my_stg_domain"
DSK_NAME = "disk1"
NIC_NAME = "nic1"
NET_NAME = "my_network"

vm = api.vms.get(VM_NAME)
stg_domain = api.storagedomains.get(STG_DOMAIN)
stg_parms = params.StorageDomains(storage_domain=[stg_domain])
# Boot disk
vm.disks.add(params.Disk(name=DSK_NAME,
                         storage_domains=stg_parms,
                         size=20*GB,
                         status=None,
                         interface='virtio',
                         format='cow',
                         sparse=False,
                         bootable=True))
wait_disk_state(DSK_NAME, "ok")

# Boot NIC
vm.nics.add(params.NIC(name=NIC_NAME,
                       network=params.Network(name=NET_NAME),
                       interface='virtio'))
boot_if = vm.nics.get(NIC_NAME).mac.address

# Add more disks and NICs to your liking...

# ---
boot_params = {"ks": "http://virtmanager.<your-domain>/ks/",
               "ksdevice": boot_if,
               "dns": "1.2.3.4,1.2.3.5",
               "ip": "10.9.8.7",
               "netmask": "255.255.255.0",
               "gateway": "10.9.8.1",
               "hostname": "{0}.my.domain".format(VM_NAME)}
cmdline = " ".join(map("{0[0]}={0[1]}".format, boot_params.iteritems()))
vm.set_os(params.OperatingSystem(kernel="iso://vmlinuz",
                                 initrd="iso://initrd.img",
                                 cmdline=cmdline))
vm.update()

# ---
vm.start()

# ---
# Wait for machine to power off
wait_vm_state(VM_NAME, "down")
# Remove boot parameters
vm.set_os(params.OperatingSystem(kernel="", initrd="", cmdline=""))
vm.update()
# Start the VM after the installation
vm.start()
api.disconnect()
EOF
chmod u+x /root/kickstart-centos7-vm.py

cat >/root/step_3_on-virthost01_add-2nd-node.sh <<EOF
#!/bin/sh
hosted-engine --set-maintenance --mode=global && hosted-engine --vm-shutdown
while :; do
 hosted-engine --vm-status | grep "Engine status" | grep "down" | grep -v "powering" && break
 hosted-engine --vm-status | grep "Engine status"
 sleep 5
done
hosted-engine --vm-status
echo -n "Continue? "; read ANSW
systemctl stop ovirt-ha-agent && systemctl stop ovirt-ha-broker && systemctl stop vdsmd
echo -n "Continue? "; read ANSW
echo "Now run /root/step_4_on_virthost02_setup-2nd-node.sh on the second node."
echo -n "Continue? "; read ANSW
echo "172.21.0.102" >> /etc/ctdb/nodes
service restart ctdb.service
gluster peer probe virthost02.<your-domain>
echo -n "Continue? "; read ANSW
for I in \$(ls /gluster); do
 gluster volume add-brick \$I replica 2 \$HOSTENAME:/gluster/\$I/brick && gluster volume heal \$I full
done
echo -n "Continue? "; read ANSW
# start replication manually
mkdir /glustermnt
for I in \$(ls /gluster); do
 mount \$HOSTNAME:\$I /glustermnt && for J in \$(ls /gluster/\$I/brick); do stat /glustermnt/\$J ; find /glustermnt/ ; done
 umount /glustermnt || break
done
df -h; echo
echo "Check these directories on virthost02:"
du -shc /gluster/*
echo
echo "Continue when replication has finished:"
for I in \$(ls /gluster); do
 gluster volume heal \$I info
done
echo "Check with \'gluster volume heal <volume> info\'."
echo -n "Continue? "; read ANSW
# set gluster quorum to 50% to ensure ovirt-engine will continue to run after virthost01 has been shut down
gluster volume set all cluster.server-quorum-ratio 50%
echo -n "Continue? "; read ANSW
systemctl start ovirt-ha-agent && systemctl start ovirt-ha-broker
hosted-engine --set-maintenance --mode=none
while :; do
 hosted-engine --vm-status | grep "Engine status" | grep "health.*good.*vm.*up.*detail.*up" | grep -v "powering" && break
 hosted-engine --vm-status | grep "Engine status"
 sleep 5
done
hosted-engine --vm-status
echo -n "Continue? "; read ANSW
echo "DONE"
EOF
chmod u+x /root/step_3_on_virthost01_add-2nd-node.sh

cat >/root/step_4_on_virthost02_setup-2nd-node.sh <<EOF
#!/bin/sh
copy -v /root/iptables.ovirt_gluster_ctdb /etc/sysconfig/iptables && systemctl reload iptables
sed -i "s/Lock=True/Lock=True\nLock=False/" /etc/nfsmount.conf
gluster volume status | grep "volume: engine" || {
 mkdir -p /gluster/{engine,images,iso,export,meta}/brick && mkdir /mnt/lock
 systemctl start glusterd && systemctl enable glusterd
 systemctl stop glusterd && rm -f /var/run/glusterd.socket && systemctl start glusterd
}
mount -t glusterfs localhost:/meta /mnt/lock
# CTDB doesn't work with SELinux enabled if config files are symlinked to /mnt/lock:
setenforce 0; sed -i "s/SELINUX=enforcing/SELINUX=permissive/g" /etc/selinux/config
mv /etc/sysconfig/ctdb /etc/sysconfig/ctdb.orig && ln -s /mnt/lock/ctdb /etc/sysconfig/ctdb && ln -s /mnt/lock/nodes /etc/ctdb/nodes && ln -s /mnt/lock/public_addresses /etc/ctdb/public_addresses && systemctl start ctdb && systemctl enable ctdb
cat >/etc/systemd/system/ctdb.service <<IEOF
[Unit]
Description=CTDB
After=mnt-lock.mount
Requires=mnt-lock.mount
Requires=glusterd.service

[Service]
Type=forking
LimitCORE=infinity
PIDFile=/run/ctdb/ctdbd.pid
ExecStart=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid start
ExecStop=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid stop
KillMode=control-group
Restart=on-failure

[Install]
WantedBy=multi-user.target
IEOF
systemctl enable ctdb.service
cat >/etc/systemd/system/mnt-lock.mount <<IEOF
[Unit]
Description=ctdb meta volume
Requires=glusterd.service
Before=ctdb.service

[Mount]
What=localhost:meta
Where=/mnt/lock
Type=glusterfs
Options=defaults,_netdev

[Install]
WantedBy=multi-user.target
IEOF
systemctl enable mnt-lock.mount
echo;echo -n "Run \"hosted-engine --deploy --config-append=/root/step_4_on_virthost02_hosted-engine-deploy.answers\" ? <CTRL-C> to cancel."; read ANSWER
hosted-engine --deploy --config-append=/root/step_4_on_virthost02_hosted-engine-deploy.answers
EOF
chmod u+x /root/step_4_on_virthost02_setup-2nd-node.sh

cat >/root/step_4_on_virthost02_hosted-engine-deploy.answers <<EOF
[environment:default]
OVEHOSTED_CORE/screenProceed=none:None
OVEHOSTED_CORE/deployProceed=bool:True
OVEHOSTED_CORE/confirmSettings=bool:True
OVEHOSTED_NETWORK/fqdn=str:virtmanager.<your-domain>
OVEHOSTED_NETWORK/bridgeName=str:ovirtmgmt
OVEHOSTED_NETWORK/firewallManager=str:iptables
OVEHOSTED_NETWORK/gateway=str:172.21.0.99
OVEHOSTED_ENGINE/clusterName=str:Default
#OVEHOSTED_STORAGE/iSCSITargetName=none:None
OVEHOSTED_STORAGE/storageDomainName=str:hosted_storage
OVEHOSTED_STORAGE/storageDatacenterName=str:hosted_datacenter
OVEHOSTED_STORAGE/storageType=none:None
#OVEHOSTED_STORAGE/lockspaceVolumeUUID=str:7b717cf7-b235-4bc3-adba-55f11fa220bd
OVEHOSTED_STORAGE/domainType=str:nfs3
#OVEHOSTED_STORAGE/iSCSIPortalPort=none:None
#OVEHOSTED_STORAGE/connectionUUID=str:9a828ded-dff8-4314-b8d0-41f75fbc0957
#OVEHOSTED_STORAGE/spUUID=str:dd08810a-46b5-4c02-8b0f-95a3cf2a76dd
#OVEHOSTED_STORAGE/lockspaceImageUUID=str:35012ab7-0176-4f75-900f-4c27e9f51d10
#OVEHOSTED_STORAGE/iSCSILunId=none:None
#OVEHOSTED_STORAGE/metadataImageUUID=str:558c667d-32f1-492c-b4d6-aa2210294b46
OVEHOSTED_STORAGE/imgAlias=str:hosted_engine
#OVEHOSTED_STORAGE/volUUID=str:5903ab50-6a75-423a-9574-f98e6349bd2f
#OVEHOSTED_STORAGE/iSCSIPortal=none:None
#OVEHOSTED_STORAGE/imgSizeGB=str:25
#OVEHOSTED_STORAGE/iSCSIPortalIPAddress=none:None
#OVEHOSTED_STORAGE/vgUUID=none:None
OVEHOSTED_STORAGE/storageDomainConnection=str:virtstorage.<your-domain>:/engine
#OVEHOSTED_STORAGE/iSCSIPortalUser=none:None
#OVEHOSTED_STORAGE/metadataVolumeUUID=str:e8c36f64-a8cd-40bb-981b-5325841923b6
#OVEHOSTED_STORAGE/imgUUID=str:0630c3d0-053d-41ca-af30-66ce5756c589
#OVEHOSTED_STORAGE/sdUUID=str:50576a61-34d6-4e27-bf65-3587b04cfc8d
#OVEHOSTED_VM/nicUUID=str:f69359c9-e478-4ab1-9d95-0345471318f9
#OVEHOSTED_VDSM/consoleType=str:vnc
#OVEHOSTED_VM/vmMemSizeMB=str:4096
#OVEHOSTED_VM/vmUUID=str:611fc68b-54e0-475b-a3f1-f044276c4a16
#OVEHOSTED_VM/vmMACAddr=str:00:16:3e:01:01:01
#OVEHOSTED_VM/emulatedMachine=str:pc
#OVEHOSTED_VM/consoleUUID=str:a1076084-56e9-4a83-8e5e-e7250fb3171f
#OVEHOSTED_VM/vmBoot=str:disk
#OVEHOSTED_VM/vmVCpus=str:2
#OVEHOSTED_VM/cdromUUID=str:90d87827-d52b-4499-b1c5-bd104d6f14b8
#OVEHOSTED_VM/ovfArchive=none:None
#OVEHOSTED_VM/vmCDRom=none:None
#OVEHOSTED_VDSM/spicePkiSubject=str:C=EN, L=Test, O=Test, CN=Test
#OVEHOSTED_VDSM/caSubject=str:/C=EN/L=Test/O=Test/CN=TestCA
#OVEHOSTED_VDSM/cpu=str:model_Penryn
#OVEHOSTED_VDSM/pkiSubject=str:/C=EN/L=Test/O=Test/CN=Test
#OVEHOSTED_NOTIF/smtpPort=str:25
#OVEHOSTED_NOTIF/smtpServer=str:localhost
#OVEHOSTED_NOTIF/sourceEmail=str:root@localhost
#OVEHOSTED_NOTIF/destEmail=str:root@localhost
EOF

cat >/root/iptables.ovirt_gluster_ctdb <<EOF
# oVirt/Gluster firewall configuration
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -i lo -j ACCEPT
# vdsm
-A INPUT -p tcp --dport 54321 -j ACCEPT
# SSH
-A INPUT -p tcp --dport 22 -j ACCEPT
# snmp
-A INPUT -p udp --dport 161 -j ACCEPT
# libvirt tls
-A INPUT -p tcp --dport 16514 -j ACCEPT
# guest consoles
-A INPUT -p tcp -m multiport --dports 5900:6923 -j ACCEPT
# migration
-A INPUT -p tcp -m multiport --dports 49152:49216 -j ACCEPT
# glusterd
-A INPUT -p tcp -m tcp --dport 24007 -j ACCEPT
# portmapper
-A INPUT -p udp -m udp --dport 111   -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38465 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38466 -j ACCEPT
# nfs
-A INPUT -p tcp -m tcp --dport 111   -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38467 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 2049  -j ACCEPT
# status
-A INPUT -p tcp -m tcp --dport 39543 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 55863 -j ACCEPT
# nlockmgr
-A INPUT -p tcp -m tcp --dport 38468 -j ACCEPT
-A INPUT -p udp -m udp --dport 963   -j ACCEPT
-A INPUT -p tcp -m tcp --dport 965   -j ACCEPT
# ctdbd
-A INPUT -p tcp -m tcp --dport 4379  -j ACCEPT
# Ports for gluster volume bricks (default 100 ports)
-A INPUT -p tcp -m tcp --dport 24009:24108 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 49152:49251 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 34865:34867 -j ACCEPT
#
# Reject any other input traffic
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -m physdev ! --physdev-is-bridged -j REJECT --reject-with icmp-host-prohibited
COMMIT
EOF

cat >/root/iptables-open-nfs-ports.sh <<EOF
# open NFS ports:
grep -B100 "A INPUT -j REJECT" /etc/sysconfig/iptables | head -n -1 > /etc/sysconfig/iptables.new
cat >> /etc/sysconfig/iptables.new <<IEOF
# portmapper
-A INPUT -p udp -m udp --dport 111   -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38465 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38466 -j ACCEPT
# nfs
-A INPUT -p tcp -m tcp --dport 111   -j ACCEPT
-A INPUT -p tcp -m tcp --dport 38467 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 2049  -j ACCEPT
IEOF
grep -A100 "A INPUT -j REJECT" /etc/sysconfig/iptables >> /etc/sysconfig/iptables.new
mv /etc/sysconfig/iptables /etc/sysconfig/iptables.\$(date +"%s")
mv /etc/sysconfig/iptables.new /etc/sysconfig/iptables
service iptables restart
EOF
chmod u+x /root/iptables-open-nfs-ports.sh

cat >/root/step_5_on_virthost01_remove-host.sh <<EOF
#!/bin/sh
# remove gluster volumes from cluster
for I in \$(ls /gluster); do
 gluster volume remove-brick \$I replica 1 virthost01.<your-domain>:/gluster/\$I/brick force
done
# CTDB cluster
echo "Remove host from virtmanager via webfrontend."
EOF
chmod u+x /root/step_5_on_virthost01_remove-host.sh

cat >/root/virtmanager-ks.cfg <<EOF
#version=RHEL7
# System authorization information
auth --enableshadow --passalgo=sha512

# Use CDROM installation media
cdrom
# Use graphical install
graphical
# Run the Setup Agent on first boot
firstboot --enable
ignoredisk --only-use=vda
# Keyboard layouts
keyboard --vckeymap=de-nodeadkeys --xlayouts='de (nodeadkeys)'
# System language
lang en_US.UTF-8

# Network information
network  --bootproto=static --device=eth0 --ip=172.21.0.100 --netmask=255.255.255.0 --ipv6=auto --activate
network  --hostname=virtmanager.<your-domain>
# Root password
rootpw 1234567890
# System services
services --enabled="chronyd"
# System timezone
timezone Europe/Berlin --isUtc
# System bootloader configuration
bootloader --location=mbr --boot-drive=vda
autopart --type=lvm
# Partition clearing information
clearpart --none --initlabel

#%packages
@core
chrony

#%end

poweroff
EOF

cat >/root/Up\ and\ Running\ with\ oVirt\ 3.5.html <<EOF
<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Up and Running with oVirt 3.5 &mdash;
Red Hat Open Source Community
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='Jason Brooks' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>
<link href='/blog/feed.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<!-- Open Graph (FB / G+) -->
<meta content='article' property='og:type'>
<meta content='Up and Running with oVirt 3.5' property='og:title'>
<meta content="Last week, version 3.5 of oVirt, the open source virtualization management system, hit FTP mirrors sporting a slate of fixes and enhancements, including a new-look user interface, and support for using CentOS 7 machines as virtualization hosts.&#x000A;&#x000A;As with every new oVirt release, I'm here to suggest a path to getting up and running with the project on single server, with an option for expanding to additional machines in the future. First, though, a quick rundown of the different single-machine options for trying out oVirt:&#x000A;&#x000A;&#x000A;  oVirt Live ISO: A LiveCD image that you can burn onto a blank CD or copy onto a USB stick to boot from and run oVirt. This is probably the fastest way to get up and running, but once you're up, this is probably your lowest-performance option, and not suitable for extended use or expansion.&#x000A;  oVirt All in One plugin: Run the oVirt management server and virtualization host components on a single machine with local storage. This is a more permanent version of the Live ISO approach, and had been my favored kick-the-tires option until the rise of…&#x000A;  oVirt Hosted Engine: The self-hosted engine approach consists of an oVirt virtualization host that serves up its own management engine. This route is a bit more complicated than those above, but I like it because:&#x000A;    &#x000A;      oVirt 3.5 supports CentOS 7 as a virtualization host, but not as a host for the management engine. Running oVirt Engine in a separate VM allows you to put CentOS 7 on your metal, and keep CentOS 6 around for the engine.&#x000A;      With the All-in-One approach, your management engine is married to the machine it's installed on, limiting your expansion options. The Hosted Engine can move among hosts.&#x000A;    &#x000A;  &#x000A;&#x000A;&#x000A;For this howto, I'll be walking through the steps you can follow to get oVirt 3.5 up and running on a single machine with a self-hosted engine, and with self-hosted storage, courtesty of GlusterFS. &#x000A;&#x000A;In my next post, I'll describe how to add two more machines to the mix to give yourself an installation hardy enough to bring a machine down for updates and maintainence without everything grinding to a halt.&#x000A;&#x000A;If you have access to good external NFS or iSCSI storage to use with your oVirt exploration, I'll point out where you can skip the GlusterFS bits and use your external storage resource." property='og:description'>
<meta content='http://community.redhat.com/blog/2014/10/up-and-running-with-ovirt-3-5/' property='og:url'>
<meta content='2014-10-29T13:00:00Z' property='article:published_time'>
<meta content='jbrooks' property='article:author:username'>
<meta content='centos' property='article:tag'>
<link href='/blog/tag/centos.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='gluster' property='article:tag'>
<link href='/blog/tag/gluster.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='ovirt' property='article:tag'>
<link href='/blog/tag/ovirt.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='virtualization' property='article:tag'>
<link href='/blog/tag/virtualization.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<!-- Twitter card -->
<meta content='summary' name='twitter:card'>
<meta content='@redhatopen' name='twitter:site'>
<meta content='Up and Running with oVirt 3.5' name='twitter:title'>
<meta content="Last week, version 3.5 of oVirt, the open source virtualization management system, hit FTP mirrors sporting a slate of fixes and enhancements, including a new-look user interface, and support for using CentOS 7 machines as virtualization hosts.&#x000A;&#x000A;As with every new oVirt release, I'm here to suggest a path to getting up and running with the project on single server, with an option for expanding to additional machines in the future. First, though, a quick rundown of the different single-machine options for trying out oVirt:&#x000A;&#x000A;&#x000A;  oVirt Live ISO: A LiveCD image that you can burn onto a blank CD or copy onto a USB stick to boot from and run oVirt. This is probably the fastest way to get up and running, but once you're up, this is probably your lowest-performance option, and not suitable for extended use or expansion.&#x000A;  oVirt All in One plugin: Run the oVirt management server and virtualization host components on a single machine with local storage. This is a more permanent version of the Live ISO approach, and had been my favored kick-the-tires option until the rise of…&#x000A;  oVirt Hosted Engine: The self-hosted engine approach consists of an oVirt virtualization host that serves up its own management engine. This route is a bit more complicated than those above, but I like it because:&#x000A;    &#x000A;      oVirt 3.5 supports CentOS 7 as a virtualization host, but not as a host for the management engine. Running oVirt Engine in a separate VM allows you to put CentOS 7 on your metal, and keep CentOS 6 around for the engine.&#x000A;      With the All-in-One approach, your management engine is married to the machine it's installed on, limiting your expansion options. The Hosted Engine can move among hosts.&#x000A;    &#x000A;  &#x000A;&#x000A;&#x000A;For this howto, I'll be walking through the steps you can follow to get oVirt 3.5 up and running on a single machine with a self-hosted engine, and with self-hosted storage, courtesty of GlusterFS. &#x000A;&#x000A;In my next post, I'll describe how to add two more machines to the mix to give yourself an installation hardy enough to bring a machine down for updates and maintainence without everything grinding to a halt.&#x000A;&#x000A;If you have access to good external NFS or iSCSI storage to use with your oVirt exploration, I'll point out where you can skip the GlusterFS bits and use your external storage resource." name='twitter:description'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css?1430415680" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css?1430415675" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo " alt="Red Hat Open Source Community" width="150" height="46" src="/images/logo.png?1430415680" />
</a></h1>
<h2 id='site-title'>
Open Source Community
</h2>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-events' role='menuitem'>
<a href='/events/'>Events</a>
</li>

<li class='nav-link-software' role='menuitem'>
<a href='/software/'>Software</a>
</li>

<li class='nav-link-standards' role='menuitem'>
<a href='/standards/'>Standards</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='container' id='page-wrap'>
<section id='page'>
<section class='container content' id='content'>
<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<section class='blog-post-page row'>
<div class='col-md-10 col-md-offset-1'>
<article class='post hentry'>
<header class='post-header'>
<h2 class='post-title entry-title'>
Up and Running with oVirt 3.5
</h2>
<header class='post-meta'>
<span class='byline'>
by
<span class='author vcard'>
<a href="/blog/author/jbrooks/">Jason Brooks</a>
</span>
</span>
&ndash;
<time class='published' datetime='2014-10-29T13:00:00Z'>
Wednesday 29 October 2014
</time>
</header>
</header>
<section class='post-content entry-content'>
<p><a href="http://www.ovirt.org"><img alt="" align="right" alt="" width="180" height="80" src="/images/blog/oVirt-logo.png?1430415680" /></a>
Last week, version 3.5 of oVirt, the open source virtualization management system, <a href="http://lists.ovirt.org/pipermail/announce/2014-October/000138.html">hit FTP mirrors</a> sporting a slate of fixes and enhancements, including a new-look user interface, and support for using CentOS 7 machines as virtualization hosts.</p>

<p>As with every new oVirt release, I'm here to suggest a path to getting up and running with the project on single server, with an option for expanding to additional machines in the future. First, though, a quick rundown of the different single-machine options for trying out oVirt:</p>

<ul>
  <li><a href="http://www.ovirt.org/OVirt_Live">oVirt Live ISO</a>: A LiveCD image that you can burn onto a blank CD or copy onto a USB stick to boot from and run oVirt. This is probably the fastest way to get up and running, but once you're up, this is probably your lowest-performance option, and not suitable for extended use or expansion.</li>
  <li><a href="http://www.ovirt.org/Feature/AllInOne">oVirt All in One plugin</a>: Run the oVirt management server and virtualization host components on a single machine with local storage. This is a more permanent version of the Live ISO approach, and <a href="/blog/2013/09/up-and-running-with-ovirt-3-3/">had been</a> my favored kick-the-tires option until the rise of…</li>
  <li><a href="http://www.ovirt.org/Features/Self_Hosted_Engine">oVirt Hosted Engine</a>: The self-hosted engine approach consists of an oVirt virtualization host that serves up its own management engine. This route is a bit more complicated than those above, but I like it because:
    <ul>
      <li>oVirt 3.5 supports CentOS 7 as a virtualization host, but not as a host for the management engine. Running oVirt Engine in a separate VM allows you to put CentOS 7 on your metal, and keep CentOS 6 around for the engine.</li>
      <li>With the All-in-One approach, your management engine is married to the machine it's installed on, limiting your expansion options. The Hosted Engine can move among hosts.</li>
    </ul>
  </li>
</ul>

<p>For this howto, I'll be walking through the steps you can follow to get oVirt 3.5 up and running on a single machine with a self-hosted engine, and with self-hosted storage, courtesty of GlusterFS. </p>

<p>In my <a href="/blog/2014/11/up-and-running-with-ovirt-3-5-part-two/">next post</a>, I'll describe how to add two more machines to the mix to give yourself an installation hardy enough to bring a machine down for updates and maintainence without everything grinding to a halt.</p>

<p>If you have access to good external NFS or iSCSI storage to use with your oVirt exploration, I'll point out where you can skip the GlusterFS bits and use your external storage resource.</p>

<p></p>

<h2 id="prerequisites">Prerequisites</h2>

<p><strong>Hardware:</strong> You’ll need a machine with at least 4GB RAM and processors with <a href="http://en.wikipedia.org/wiki/X86_virtualization#Hardware-assisted_virtualization">hardware virtualization extensions</a>. A physical machine is best, but you can test oVirt using <a href="http://community.redhat.com/testing-ovirt-3-3-with-nested-kvm/">nested KVM</a> as well.</p>

<p><strong>Software:</strong> For this howto, I'm using CentOS 7 for the host and CentOS 6 for the Engine VM. oVirt does support other OS options. For more info see the project's <a href="http://www.ovirt.org/Download">download page</a>.</p>

<p><strong>Network:</strong> Your test machine’s host name must resolve properly, either through your network’s DNS, or through the /etc/hosts file on your virt host(s), on the VM that will host the oVirt engine, and on any clients from which you plan on administering oVirt.</p>

<p><strong>Storage:</strong> The hosted engine feature requires NFS or iSCSI storage to house the VM that will host the engine. For this walkthrough, I'll be using a Gluster-based NFS share hosted from my test machine. If you prefer to use external iSCSI or NFS storage, that'll work, too.</p>

<h2 id="installing-ovirt-with-hosted-engine">Installing oVirt with hosted engine</h2>

<p>I'm starting out with a test machine with 8 GB of RAM and 4 processor cores, running a minimal installation of CentOS 7 with all updates applied. </p>

<p>I've identified a pair of static IP address on my network to use for this test (one for my first host, and one for the hosted engine), and I've edited the /etc/hosts file on my test machine so that these addresses will resolve properly. </p>

<p>I'm also adding a second host name, <code>ovirt-mount.osas.lab</code>, to use as my NFS mount point – this will come in handy for my next post, when I'll have multiple machines sharing the NFS-hosting duties over a common virtual IP:</p>

<pre><code class="highlight plaintext">10.10.10.1 ovirt-one.osas.lab ovirtmount.osas.lab&#x000A;10.10.10.2 ovirt-test.osas.lab&#x000A;</code></pre>
<p>The oVirt 3.5 release notes suggest disabling NetworkManager and firewalld in favor of the tried and true network and iptables services:</p>

<pre><code class="highlight plaintext"># systemctl disable firewalld &amp;&amp; systemctl stop firewalld&#x000A;# systemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager&#x000A;</code></pre>

<p>Next, we need to configure the oVirt software repository on the first host:</p>

<pre><code class="highlight plaintext"># yum localinstall -y http://resources.ovirt.org/pub/yum-repo/ovirt-release35.rpm&#x000A;</code></pre>
<p>Next, install the hosted engine packages, along with <a href="http://www.gnu.org/software/screen/">screen</a>, which can come in handy during the deployment process:</p>

<pre><code class="highlight plaintext"># yum install -y ovirt-hosted-engine-setup screen glusterfs-server nfs-utils vdsm-gluster system-storage-manager&#x000A;</code></pre>

<p><em>I'm experiencing an SELinux issue in which glusterd isn't functional until after a reboot, so go ahead and reboot after installing these packages.</em></p>

<h2 id="gluster-preparations">Gluster preparations</h2>

<p>We need a partition to store our Gluster bricks. For simplicity, I'm using a single XFS partition, and my Gluster bricks will be directories within this partition. I use system-storage-manager to manage my storage.</p>

<p><em>If you're skipping the local Gluster storage, and using external NFS or iSCSI storage, you can skip this step.</em> </p>

<pre><code class="highlight plaintext"># ssm add -p gluster \$YOUR_DEVICE_NAME&#x000A;# ssm create -p gluster --fstype xfs -n gluster&#x000A;</code></pre>

<p>Next, modify your <code>/etc/fstab</code> to add the new partition:</p>

<pre><code class="highlight plaintext"># mkdir /gluster&#x000A;# blkid /dev/gluster/gluster&#x000A;</code></pre>

<p>Edit your <code>/etc/fstab</code> and add a line like the one below before running <code>mount -a</code> to mount your new partition:</p>

<pre><code class="highlight plaintext">UUID=\$YOUR_UUID /gluster xfs defaults 0 0&#x000A;</code></pre>

<p>Next, we'll create some mount points for our Gluster volumes-to-be. We'll have separate Gluster volumes for our hosted engine, and for our oVirt data domain:</p>

<pre><code class="highlight plaintext"># mkdir -p /gluster/{engine,data}/brick&#x000A;</code></pre>

<p>Now start the Gluster service and configure it to auto-start after subsequent reboots:</p>

<pre><code class="highlight plaintext"># systemctl start glusterd &amp;&amp; systemctl enable glusterd&#x000A;</code></pre>

<p>Next, we create our gluster volumes, substituting your machine's hostname:</p>

<pre><code class="highlight plaintext"># gluster volume create engine \$HOSTNAME:/gluster/engine/brick&#x000A;# gluster volume create data \$HOSTNAME:/gluster/data/brick&#x000A;</code></pre>

<p>Now, apply a set of virt-related volume options to our engine and data volumes:</p>

<pre><code class="highlight plaintext"># gluster volume set engine group virt&#x000A;# gluster volume set data group virt&#x000A;</code></pre>

<p>We also need to set the correct permissions on all our volumes:</p>

<pre><code class="highlight plaintext">#  gluster volume set engine storage.owner-uid 36 &amp;&amp; gluster volume set engine storage.owner-gid 36&#x000A;# gluster volume set data storage.owner-uid 36 &amp;&amp; gluster volume set data storage.owner-gid 36&#x000A;</code></pre>

<p>Finally, we need to start our volumes:</p>

<pre><code class="highlight plaintext"># gluster volume start engine&#x000A;# gluster volume start data&#x000A;</code></pre>

<p>Before moving ahead, run <code>gluster volume status</code> to make sure that the NFS server associated with your <code>engine</code> volume is up and running, because we're about to need it.</p>

<p>Due to a conflict between Gluster's built-in NFS server and NFS client-locking, it's necessary to disable file locking in the <code>/etc/nfsmount.conf</code> file with the line <code>Lock=False</code> to ensure that Gluster will reliably both serve up and access the engine volume over NFS.</p>

<h2 id="installing-the-hosted-engine">Installing the hosted engine</h2>

<p>PXE, ISO image, and OVF-formatted disk image are our installation media options for the VM that will host our engine. Here, I'm using an ISO image, and creating a temporary directory to which oVirt will have access to house the image during the install process:</p>

<pre><code class="highlight plaintext"># mkdir /home/tmp &amp;&amp; cd /home/tmp&#x000A;# curl -O http://mirrors.kernel.org/centos/6.6/isos/x86_64/CentOS-6.6-x86_64-minimal.iso&#x000A;# chown -R 36:36 /home/tmp&#x000A;</code></pre>

<p>Now we should be ready to fire up <code>screen</code> and kick off the installation process:</p>

<pre><code class="highlight plaintext"># screen&#x000A;# hosted-engine --deploy&#x000A;</code></pre>

<p>Follow along with the script, answering its questions. The default answers are fine, but you'll need to supply the path to your NFS share, the type and path to the media you'll be using to install your engine-hosting VM, the host name you've picked out for the hosted engine, and the password you'll be using for the engine admin user.</p>

<p>My NFS path looks like <code>ovirtmount.osas.lab:/engine</code>. I'm using that secondary host name I mentioned earlier in the howto, the hosting of which will, in the next post, be shared among my virt+storage machines.</p>

<p><img alt="" align="center" alt="" src="/images/blog/ovirt35-deploy-host-1.png" /></p>

<p>Once you've supplied all these answers, and confirmed your choices, the installer will launch a VM and provide you with an address and password for accessing the VM with the vnc client of your choice. Fire up a vnc client, enter the address provided and enter the password provided to access the VM.</p>

<p>From here, you can click your way through the installation of your engine-hosting VM. You can configure the VM's correct host name and IP address either now, or after you boot into the VM for the first time. As for software choices, you can simply opt for a "minimal" installation.</p>

<p>When the OS installation on your new VM is complete, head back to the terminal window where <code>hosted-engine --deploy</code> is running, and hit enter to let the script know that you're ready for the next step.</p>

<p><img alt="" align="center" alt="" width="601" height="134" src="/images/blog/ovirt34-deploy-host-1b.png?1430415680" /></p>

<p>The VM will reboot, and when it's back up, it's time to install oVirt engine. Either through vnc or through an ssh session (ssh is nicer for copying and pasting commands), access your newly-created VM, and ensure that everything is in order.</p>

<ul>
  <li>If needed, modify /etc/hosts in the new VM with the same IP address / host name mappings we set up on the first host.</li>
  <li>If you didn't take care of it during the OS installation, fix your network settings to give the VM the correct host name and IP address, as well.</li>
  <li>Apply all updates to the VM, and reboot if needed. If you do reboot the VM, go back to the terminal where you're running <code>hosted-engine --deploy</code> and select option 2, "Power off and restart the VM."</li>
</ul>

<p>Now, just as we did on our first host, we need to configure the oVirt software repository on our hosted engine VM:</p>

<pre><code class="highlight plaintext"># yum localinstall -y http://resources.ovirt.org/pub/yum-repo/ovirt-release35.rpm&#x000A;</code></pre>

<p>Next, we'll install and then set up ovirt-engine:</p>

<pre><code class="highlight plaintext"># yum install -y ovirt-engine&#x000A;# engine-setup&#x000A;</code></pre>

<p><img alt="" align="center" alt="" src="/images/blog/ovirt35-configure-engine-1.png" /></p>

<p>Go through the engine-setup script, answering its questions. You'll be fine accepting all the default answers, but make sure to supply the same admin password that you chose earlier, while running <code>hosted-engine --deploy</code>.</p>

<p>When the installation process completes, head back to the terminal where you're running the hosted engine installer and hit enter to indicate that the engine configuration is complete.</p>

<p>The installer will register itself as a virtualization host on the oVirt engine instance we've just installed. Once this completes, the installer will tell you to shut down your VM so that the ovirt-engine-ha services on the first host can restart the engine VM as a monitored service.</p>

<p><img alt="" align="center" alt="" src="/images/blog/ovirt35-configure-engine-1a.png" /></p>

<p>It can take a few minutes for the HA services to notice that the engine is down, to check that there's a machine available to host the engine, and to start up the hosted engine VM. You can watch these services do their thing by tailing their log files:</p>

<pre><code class="highlight plaintext"># tail -f /var/log/ovirt-hosted-engine-ha/*&#x000A;</code></pre>

<p>Once that process is complete, the script will exit and you should be ready to configure storage and run a VM.</p>

<h2 id="configuring-storage">Configuring storage</h2>

<p>Head to your oVirt engine console at the address of your hosted engine VM, log in with the user name <code>admin</code> and the password you chose during setup, and visit the "Storage" tab in the console.</p>

<p>Click "New Domain," give your new domain a name, and choose Data / GlusterFS from the "domain function / storage type" drop down menu. (If you're using a different sort of external storage, you can choose an option matching that, instead.)</p>

<p>In the "Export Path" field, enter the remote path to your Gluster volume, and hit the OK button to proceed. It'll take a bit of time for your new storage domain to initialize and come online, but once it does, you'll be ready to launch your first VM.</p>

<p><img alt="" align="center" alt="" src="/images/blog/ovirt35-configure-storage-1.png" /></p>

<h2 id="running-your-first-vm">Running your first VM</h2>

<p>Since version 3.4, oVirt engine has come pre-configured with a public Glance instance managed by the oVirt project. We'll tap this resource to launch our first VM.</p>

<p>From the storage tab, you should see an "ovirt-image-repository" entry next to a little OpenStack logo. Clicking on this domain will bring up a menu of images available in this repository. Click on the "CirrOS" image (which is very small and perfect for testing) in the list and then click "Import," before hitting the OK button in the pop-up dialog to continue.</p>

<p><img alt="" align="center" alt="" width="600" height="370" src="/images/blog/ovirt34-run-vm-1.png?1430415680" /></p>

<p>The image will be copied from the oVirt project's public Glance repository to the storage domain you just configured, where it will be available as a disk to attach to a new VM. In the import image dialog, you have the option of clicking the "Import as Template" check box to give yourself the option of basing multiple future VMs on this image using oVirt's templates functionality.</p>

<p>Next, head to the "Virtual Machines" tab in the console, click "New VM," choose "Linux" from the "Operating System" drop down menu, supply a name for your VM, and choose the "ovirtmgmt/ovirtmgmt" network in the drop down menu next to "nic1" before hitting the "OK" button. For additional configuration, such as setting RAM and CPU values and using cloud-init, there's a "Show Advanced Options" button in the dialog, but you can revisit that later.</p>

<p><img alt="" align="center" alt="" src="/images/blog/ovirt35-run-vm-1a.png" /></p>

<p>Next you'll get a "Guide Me" dialog box that will ask you to configure a virtual disk. Click the "Configure Virtual Disks" button, check the "Attach Disk" box at the upper left part of the dialog, select the Glance disk image we just downloaded, and hit the "OK" button to continue. Dismiss the "Guide Me" dialog by hitting the "Configure Later" button.</p>

<p><img alt="" align="center" alt="" width="600" height="240" src="/images/blog/ovirt34-run-vm-1b.png?1430415680" /></p>

<p><img alt="" align="center" alt="" width="600" height="269" src="/images/blog/ovirt34-run-vm-1c.png?1430415680" /></p>

<p>Now, back at the Virtual Machines list, right-click your new VM, and choose "Run" from the menu. After a few moments, the status of your new VM will switch from red to green, and you'll be able to click on the green monitor icon next to “Migrate” to open a console window and access your VM.</p>

<p><img alt="" align="center" alt="" width="600" height="418" src="/images/blog/ovirt34-run-vm-1d.png?1430415680" /></p>

<h2 id="till-next-time">Till next time</h2>

<p>That's enough for this post. If you run into trouble following this walkthrough, I’ll be happy to help you get up and running or get pointed in the right direction. On IRC, I’m jbrooks, ping me in the #ovirt room on OFTC, write a comment below, or give me a shout on Twitter <a href="https://twitter.com/jasonbrooks">@jasonbrooks</a>.</p>

<p>Stay tuned for a <a href="/blog/2014/11/up-and-running-with-ovirt-3-5-part-two/">followup post</a> about adding two more machines to our oVirt+Gluster setup, so that you can bring down one of the machines for maintenance or upgrades without having to shut the whole thing down.</p>

<p>If you’re interested in getting involved with the oVirt Project, you can find all the mailing list, issue tracker, source repository, and wiki information you need <a href="http://www.ovirt.org/Community">here</a>.</p>

<p>Finally, be sure to follow us on Twitter at <a href="https://twitter.com/redhatopen">@redhatopen</a> for news on oVirt and other open source projects in the Red Hat world.</p>

</section>
<footer class='post-meta'>
<div class='tags'>
Tags:
<ul class='taglist'></ul>
<li><a href="/blog/tag/centos/">centos</a></li>
<li><a href="/blog/tag/gluster/">gluster</a></li>
<li><a href="/blog/tag/ovirt/">ovirt</a></li>
<li><a href="/blog/tag/virtualization/">virtualization</a></li>
</div>
</footer>
</article>

<section id='blog-comments'></section>
</div>
</section>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<ul class='footer-nav-list'>
<li><a target="_blank" href="http://www.redhat.com/legal/privacy_statement.html">Privacy Policy</a></li>
<li><a target="_blank" href="http://www.redhat.com/legal/legal_statement.html">Terms of Use</a></li>
<li><a target="_blank" href="http://www.redhat.com/about/contact/">Contact Us</a></li>
<li><a target="_blank" href="http://www.redhat.com/about/">About Red Hat</a></li>
</ul>

Copyright &copy; 2012 &ndash; 2013 Red Hat, Inc.
</footer>


<script src="/javascripts/application.js?1430415687" type="text/javascript"></script>
<script>
  (function() {
    \$('article.post').copyright({
      text: '<hr><p>This article originally appeared on <a href="http://community.redhat.com/">community.redhat.com</a>. Follow the community on Twitter at <a href="https://twitter.com/redhatopen">@redhatopen</a>, and find us on <a href="https://www.facebook.com/redhatopen">Facebook</a> and <a href="https://plus.google.com/u/0/b/113258037797946990391/113258037797946990391/posts">Google+</a>.</p> '
    });
  
  }).call(this);
</script>

<!-- eloqua -->
<script src='http://www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script src='http://www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>
<!-- piwik -->
        <script type="text/javascript">
        var _paq = _paq || [];
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
        var u=(("https:" == document.location.protocol) ? "https" : "http") + "://piwik-osasteam.rhcloud.com/piwik/";
        _paq.push(['setTrackerUrl', u+'piwik.php']);
        _paq.push(['setSiteId', 3]);
        var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
        g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
        })();
        </script>
<noscript><p><img src="https://piwik-osasteam.rhcloud.com/piwik/piwik.php?idsite=3" style="border:0;" alt="" /></p></noscript>

</body>
</html>
EOF

cat >/root/Up\ and\ Running\ with\ oVirt\ 3.5\ -\ Part\ Two.html <<EOF
<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Up and Running with oVirt 3.5, Part Two &mdash;
Red Hat Open Source Community
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='Jason Brooks' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>
<link href='/blog/feed.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<!-- Open Graph (FB / G+) -->
<meta content='article' property='og:type'>
<meta content='Up and Running with oVirt 3.5, Part Two' property='og:title'>
<meta content="Two weeks ago in this space, I wrote about how to deploy the virtualization, storage, and management elements of the new oVirt 3.5 release on a single machine. Today, we're going to add two more machines to the mix, which will enable us to bring down one machine at a time for maintenance while allowing the rest of the deployment to continue its virtual machine hosting duties uninterrupted.&#x000A;&#x000A;We'll be configuring two more machines to match the system we set up in part one, installing and configuring CTDB to provide HA failover for the nfs share where the hosted engine lives, and expanding our single brick gluster volumes to replicated volumes that will span all three of our hosts.&#x000A;&#x000A;Before proceeding, I'll say that this converged virtualization and storage scenario is a leading-edge sort of thing. Many of the ways you might use oVirt and Gluster are available in commerically-supported configurations using RHEV and RHS, but at this time, this sort of oVirt+Gluster mashup isn't one of them. With that said, my test lab has been set up like this for the past six or seven months, and it's worked reliably for me." property='og:description'>
<meta content='http://community.redhat.com/blog/2014/11/up-and-running-with-ovirt-3-5-part-two/' property='og:url'>
<meta content='2014-11-13T14:00:00Z' property='article:published_time'>
<meta content='jbrooks' property='article:author:username'>
<meta content='centos' property='article:tag'>
<link href='/blog/tag/centos.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='gluster' property='article:tag'>
<link href='/blog/tag/gluster.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='glusterization' property='article:tag'>
<link href='/blog/tag/glusterization.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='ovirt' property='article:tag'>
<link href='/blog/tag/ovirt.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='storage' property='article:tag'>
<link href='/blog/tag/storage.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<meta content='virtualization' property='article:tag'>
<link href='/blog/tag/virtualization.xml' rel='alternate' title='Atom feed' type='application/atom+xml'>
<!-- Twitter card -->
<meta content='summary' name='twitter:card'>
<meta content='@redhatopen' name='twitter:site'>
<meta content='Up and Running with oVirt 3.5, Part Two' name='twitter:title'>
<meta content="Two weeks ago in this space, I wrote about how to deploy the virtualization, storage, and management elements of the new oVirt 3.5 release on a single machine. Today, we're going to add two more machines to the mix, which will enable us to bring down one machine at a time for maintenance while allowing the rest of the deployment to continue its virtual machine hosting duties uninterrupted.&#x000A;&#x000A;We'll be configuring two more machines to match the system we set up in part one, installing and configuring CTDB to provide HA failover for the nfs share where the hosted engine lives, and expanding our single brick gluster volumes to replicated volumes that will span all three of our hosts.&#x000A;&#x000A;Before proceeding, I'll say that this converged virtualization and storage scenario is a leading-edge sort of thing. Many of the ways you might use oVirt and Gluster are available in commerically-supported configurations using RHEV and RHS, but at this time, this sort of oVirt+Gluster mashup isn't one of them. With that said, my test lab has been set up like this for the past six or seven months, and it's worked reliably for me." name='twitter:description'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css?1430415680" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css?1430415675" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo " alt="Red Hat Open Source Community" width="150" height="46" src="/images/logo.png?1430415680" />
</a></h1>
<h2 id='site-title'>
Open Source Community
</h2>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-events' role='menuitem'>
<a href='/events/'>Events</a>
</li>

<li class='nav-link-software' role='menuitem'>
<a href='/software/'>Software</a>
</li>

<li class='nav-link-standards' role='menuitem'>
<a href='/standards/'>Standards</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='container' id='page-wrap'>
<section id='page'>
<section class='container content' id='content'>
<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<section class='blog-post-page row'>
<div class='col-md-10 col-md-offset-1'>
<article class='post hentry'>
<header class='post-header'>
<h2 class='post-title entry-title'>
Up and Running with oVirt 3.5, Part Two
</h2>
<header class='post-meta'>
<span class='byline'>
by
<span class='author vcard'>
<a href="/blog/author/jbrooks/">Jason Brooks</a>
</span>
</span>
&ndash;
<time class='published' datetime='2014-11-13T14:00:00Z'>
Thursday 13 November 2014
</time>
</header>
</header>
<section class='post-content entry-content'>
<p><a href="http://www.ovirt.org"><img alt="" align="right" alt="" width="180" height="80" src="/images/blog/oVirt-logo.png?1430415680" /></a>
Two weeks ago in this space, I <a href="/blog/2014/10/up-and-running-with-ovirt-3-5/">wrote about</a> how to deploy the virtualization, storage, and management elements of the new oVirt 3.5 release on a single machine. Today, we're going to add two more machines to the mix, which will enable us to bring down one machine at a time for maintenance while allowing the rest of the deployment to continue its virtual machine hosting duties uninterrupted.</p>

<p>We'll be configuring two more machines to match the system we set up in <a href="/blog/2014/10/up-and-running-with-ovirt-3-5/">part one</a>, installing and configuring <a href="https://ctdb.samba.org/">CTDB</a> to provide HA failover for the nfs share where the hosted engine lives, and expanding our single brick gluster volumes to replicated volumes that will span all three of our hosts.</p>

<p>Before proceeding, I'll say that this converged virtualization and storage scenario is a leading-edge sort of thing. Many of the ways you might use oVirt and Gluster are available in commerically-supported configurations using RHEV and RHS, but at this time, this sort of oVirt+Gluster mashup isn't one of them. With that said, my test lab has been set up like this for the past six or seven months, and it's worked reliably for me.</p>

<p></p>

<h2 id="prerequisites">Prerequisites</h2>

<p>The hardware and software prerequisites are the same as for the <a href="/blog/2014/10/up-and-running-with-ovirt-3-5/">Up and Running with oVirt 3.5</a> walkthrough. In addition to the system we set up last time, you'll need two more machines running minimal installs of CentOS 7. </p>

<p>For networking, you can get away with a single network adapter, but for best results, you'll want three: one for the CTDB heartbeat, one for Gluster traffic, and one for oVirt management traffic and everything else. No matter how you arrange your networking, your three hosts will need to be able to reach other on your network(s). If need be, edit <code>/etc/hosts</code> on your machines to establish the right ip address / host name mappings.</p>

<p class="alert alert-warning"><strong>NOTE:</strong> There are a few spots in this setup where I'm still tracking down SELinux issues, so, for now, this howto requires that SELinux be in <em>permissive</em> mode. On <strong>all three</strong> of your hosts, run <code>setenforce 0</code> and edit <code>/etc/selinux/config</code> and change <code>SELINUX=enforcing</code> to <code>SELINUX=permissive</code> to make the setting stick.</p>

<h2 id="shut-down-your-engine">Shut down your engine</h2>

<p>First, if you're following along from Part One, and have a running hosted engine, turn that off for now by putting the engine into maintenance mode:</p>

<pre><code class="highlight plaintext"># hosted-engine --set-maintenance --mode=global&#x000A;</code></pre>

<p>And then by either logging into your hosted engine VM and shutting it off with something like <code>shutdown -P now</code>, or, from your first host, with <code>hosted-engine --vm-shutdown</code> or with the less subtle<code>hosted-engine --vm-poweroff</code>.</p>

<p>Then stop the following services:</p>

<pre><code class="highlight plaintext"># systemctl stop ovirt-ha-agent &amp;&amp; systemctl stop ovirt-ha-broker &amp;&amp; systemctl stop vdsmd&#x000A;</code></pre>

<p>You should end up with your hosted engine volume unmounted (you can check with <code>mount</code>). This is important because we're going to change the IP address it's mounted at from the address for our first host to a new, virtual IP.</p>

<h2 id="setting-up-the-additional-pair-of-hosts">Setting up the additional pair of hosts</h2>

<p>For convenience, I'm going to smush together as many of the steps (already covered in part one) needed to prepare our two additional CentOS 7 minimal machines to join our installation as possible. On machines <strong>two and three</strong>, you need to:</p>

<pre><code class="highlight plaintext"># systemctl disable firewalld &amp;&amp; systemctl enable iptables &amp;&amp; systemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager &amp;&amp; yum localinstall -y http://resources.ovirt.org/pub/yum-repo/ovirt-release35.rpm &amp;&amp; yum install -y ovirt-hosted-engine-setup screen glusterfs-server nfs-utils vdsm-gluster system-storage-manager ctdb &amp;&amp; systemctl reboot&#x000A;</code></pre>

<h2 id="configure-your-firewall">Configure your firewall</h2>

<p>I left this step out of Part One, because oVirt's default firewall configuration worked "out of the box" there, but for this configuration, we'll need to update the firewall configuration on <strong>all three</strong> of our machines.</p>

<p>Edit <code>/etc/sysconfig/iptables</code> to include the rules you'll need for Gluster, oVirt and CTDB:</p>

<pre><code class="highlight plaintext"># oVirt/Gluster firewall configuration&#x000A;*filter&#x000A;:INPUT ACCEPT [0:0]&#x000A;:FORWARD ACCEPT [0:0]&#x000A;:OUTPUT ACCEPT [0:0]&#x000A;-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT&#x000A;&#x000A;-A INPUT -i lo -j ACCEPT&#x000A;&#x000A;# vdsm&#x000A;-A INPUT -p tcp --dport 54321 -j ACCEPT&#x000A;&#x000A;# SSH&#x000A;-A INPUT -p tcp --dport 22 -j ACCEPT&#x000A;&#x000A;# snmp&#x000A;-A INPUT -p udp --dport 161 -j ACCEPT&#x000A;&#x000A;# libvirt tls&#x000A;-A INPUT -p tcp --dport 16514 -j ACCEPT&#x000A;&#x000A;# guest consoles&#x000A;-A INPUT -p tcp -m multiport --dports 5900:6923 -j ACCEPT&#x000A;&#x000A;# migration&#x000A;-A INPUT -p tcp -m multiport --dports 49152:49216 -j ACCEPT&#x000A;&#x000A;# glusterd&#x000A;-A INPUT -p tcp -m tcp --dport 24007 -j ACCEPT&#x000A;&#x000A;# portmapper&#x000A;-A INPUT -p udp -m udp --dport 111   -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 38465 -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 38466 -j ACCEPT&#x000A;&#x000A;# nfs&#x000A;-A INPUT -p tcp -m tcp --dport 111   -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 38467 -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 2049  -j ACCEPT&#x000A;&#x000A;# status&#x000A;-A INPUT -p tcp -m tcp --dport 39543 -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 55863 -j ACCEPT&#x000A;&#x000A;# nlockmgr&#x000A;-A INPUT -p tcp -m tcp --dport 38468 -j ACCEPT&#x000A;-A INPUT -p udp -m udp --dport 963   -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 965   -j ACCEPT&#x000A;&#x000A;# ctdbd&#x000A;-A INPUT -p tcp -m tcp --dport 4379  -j ACCEPT&#x000A;&#x000A;# Ports for gluster volume bricks (default 100 ports)&#x000A;-A INPUT -p tcp -m tcp --dport 24009:24108 -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 49152:49251 -j ACCEPT&#x000A;-A INPUT -p tcp -m tcp --dport 34865:34867 -j ACCEPT&#x000A;&#x000A;# Reject any other input traffic&#x000A;-A INPUT -j REJECT --reject-with icmp-host-prohibited&#x000A;-A FORWARD -m physdev ! --physdev-is-bridged -j REJECT --reject-with icmp-host-prohibited&#x000A;COMMIT&#x000A;</code></pre>
<p>Reload your iptables service:</p>

<pre><code class="highlight plaintext"># systemctl reload iptables&#x000A;</code></pre>

<h2 id="gluster-preparations">Gluster preparations</h2>

<p>Again, a smushed-together version of the storage-setup steps I covered in part one. Assuming a new storage device for use with Gluster on each of your machines, named <code>/dev/vdb</code> (change to fit your environment), the commands would be:</p>

<pre><code class="highlight plaintext"># mkdir /gluster &amp;&amp; ssm create -p gluster --fstype xfs -n gluster /gluster /dev/vdb &amp;&amp; mkdir -p /gluster/{engine,data,meta}/brick &amp;&amp; mkdir /mnt/lock &amp;&amp; systemctl start glusterd &amp;&amp; systemctl enable glusterd &amp;&amp; blkid /dev/gluster/gluster &#x000A;</code></pre>

<p>Take the UUID from above and edit it into your <code>/etc/fstab</code>(s) with a line like the one below:</p>

<pre><code class="highlight plaintext">UUID=\$YOUR_UUID /gluster xfs defaults 0 0&#x000A;</code></pre>

<p>Now, we should be ready to add our two new machines to the Gluster trusted pool, combining them into a single Gluster trusted pool. </p>

<pre><code class="highlight plaintext"># gluster peer probe \$YOUR_SECOND_MACHINE&#x000A;# gluster peer probe \$YOUR_THIRD_MACHINE&#x000A;</code></pre>

<p>Next, we'll convert our single machine, single brick <code>engine</code> and <code>data</code> volumes to replica three volumes that span all three hosts:</p>

<pre><code class="highlight plaintext"># gluster volume add-brick engine replica 3 \$YOUR_SECOND_MACHINE:/gluster/engine/brick \$YOUR_THIRD_MACHINE:/gluster/engine/brick&#x000A;# gluster volume add-brick data replica 3 \$YOUR_SECOND_MACHINE:/gluster/data/brick \$YOUR_THIRD_MACHINE:/gluster/data/brick&#x000A;</code></pre>

<p>During my tests, I found either that Gluster either wasn't replicating the data from my initial first-host brick over to the new pair of hosts I added on its own, even after I issued the <code>gluster volume heal engine full</code> command that should have spurred replication. I managed to force the sync, however.</p>

<p>By running <code>ls /gluster/engine/brick/</code> on my <strong>first host</strong>, I saw the directory and file contained in my engine volume:</p>

<pre><code class="highlight plaintext">de38fb3c-6eb4-4241-9ca8-45793d864033 __DIRECT_IO_TEST__&#x000A;</code></pre>

<p>I switched to my <strong>second host</strong>, created a temporary mount point, mounted the engine volume, and ran <code>stat</code> on that file and directory:</p>

<pre><code class="highlight plaintext"># mkdir tmpmnt&#x000A;# mount localhost:engine tmpmnt&#x000A;# stat tmpmnt/de38fb3c-6eb4-4241-9ca8-45793d864033&#x000A;# stat tmpmnt/__DIRECT_IO_TEST__&#x000A;# umount tmpmnt&#x000A;</code></pre>

<p>Finally, due to a conflict between Gluster's built-in NFS server and NFS client-locking, it's necessary to disable file locking in the <code>/etc/nfsmount.conf</code> file with the line <code>Lock=False</code> to ensure that Gluster will reliably both serve up and access the <code>engine</code> volume over NFS. Make this configuration change on <strong>all three</strong> machines.</p>

<h2 id="ctdb-configuration">CTDB configuration</h2>

<p>We need a new Gluster volume to use with CTDB. Make a new brick directory on your <strong>first host</strong>, create and start the volume, and then create a mount point and mount the volume locally. There's no need to mess with the <code>stat</code> workaround here, because (I think) this volume is beginning life as a replicated volume.</p>

<pre><code class="highlight plaintext"># mkdir -p /gluster/meta/brick&#x000A;# gluster volume create meta replica 3 \$YOUR_FIRST_MACHINE:/gluster/meta/brick \$YOUR_SECOND_MACHINE:/gluster/meta/brick \$YOUR_THIRD_MACHINE:/gluster/meta/brick &#x000A;# gluster volume start meta&#x000A;# mkdir -p /mnt/lock&#x000A;# mount -t glusterfs localhost:/meta /mnt/lock&#x000A;</code></pre>

<p>We also need to install ctdb on our <strong>first host</strong>:</p>

<pre><code class="highlight plaintext"># yum install ctdb -y&#x000A;</code></pre>

<p>Next, we'll set up the configuration files for ctdb. Still on your <strong>first host</strong>, start by editing <code>/mnt/lock/ctdb</code>:</p>

<pre><code class="highlight plaintext">CTDB_PUBLIC_ADDRESSES=/mnt/lock/public_addresses&#x000A;CTDB_NODES=/etc/ctdb/nodes&#x000A;# Only when using Samba. Unnecessary for NFS.&#x000A;CTDB_MANAGES_SAMBA=no&#x000A;# some tunables&#x000A;CTDB_SET_DeterministicIPs=1&#x000A;CTDB_SET_RecoveryBanPeriod=120&#x000A;CTDB_SET_KeepaliveInterval=5&#x000A;CTDB_SET_KeepaliveLimit=5&#x000A;CTDB_SET_MonitorInterval=15&#x000A;CTDB_RECOVERY_LOCK=/mnt/lock/reclock&#x000A;</code></pre>

<p>Edit <code>/mnt/lock/nodes</code> to include the list of CTDB interconnect/heartbeat IPs. For our three-node install there'll be three of these. For more info on CTDB configuration, see <a href="https://ctdb.samba.org/configuring.html">Configuring CTDB</a>.</p>

<p>Next, edit <code>/mnt/lock/public_addresses</code> to include the list of virtual addresses to be hosted between the three machines (we only need one), and the network range, and the nic we're using to host this virtual address:</p>

<pre><code class="highlight plaintext">XX.XX.XX.XX/24 eth0&#x000A;</code></pre>

<p>in part one of this howto, we created a host name to use for mounting our Gluster-hosted NFS share (I called mine <code>ovirtmount.osas.lab</code>) and associated that host name with the IP address of our first host. Now that we're almost ready to hand over hosting duties for that role to CTDB, we need to change our DNS or <code>/etc/hosts</code> to associate this extra host name with the virtual address specified in <code>/mnt/lock/public_addresses</code> above.</p>

<p>Now, we'll mount the <code>meta</code> volume, and point our CTDB configuration files at the files we've created in the shared <code>meta</code> volume. Run this series of steps on your <strong>first machine</strong> :</p>

<pre><code class="highlight plaintext"># mv /etc/sysconfig/ctdb /etc/sysconfig/ctdb.orig &amp;&amp; ln -s /mnt/lock/ctdb /etc/sysconfig/ctdb &amp;&amp; ln -s /mnt/lock/nodes /etc/ctdb/nodes &amp;&amp; ln -s /mnt/lock/public_addresses /etc/ctdb/public_addresses &amp;&amp; systemctl start ctdb &amp;&amp; systemctl enable ctdb&#x000A;</code></pre>

<p>Then, on machines <strong>two and three</strong>, run <code>mount -t glusterfs localhost:/meta /mnt/lock</code> followed by the string of commands above.</p>

<p>You can check the status of ctdb by running <code>ctdb status</code>, or <code>systemctl status ctdb</code>. </p>

<p>Following future reboots, we'll want ctdb to start after our <code>meta</code> volume is mounted, which depends on Gluster being up and running. If the service fails for some reason, we want it to start back up. On <strong>all three</strong> machines, create  <code>/etc/systemd/system/ctdb.service</code>  to ask systemd to make it so:</p>

<pre><code class="highlight plaintext">[Unit]&#x000A;Description=CTDB&#x000A;After=mnt-lock.mount&#x000A;Requires=mnt-lock.mount&#x000A;Requires=glusterd.service&#x000A;&#x000A;[Service]&#x000A;Type=forking&#x000A;LimitCORE=infinity&#x000A;PIDFile=/run/ctdb/ctdbd.pid&#x000A;ExecStart=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid start&#x000A;ExecStop=/usr/sbin/ctdbd_wrapper /run/ctdb/ctdbd.pid stop&#x000A;KillMode=control-group&#x000A;Restart=on-failure&#x000A;&#x000A;[Install]&#x000A;WantedBy=multi-user.target&#x000A;</code></pre>

<p>Then, edit <code>/etc/systemd/system/mnt-lock.mount</code> to handle the <code>meta</code> volume mounting:</p>

<pre><code class="highlight plaintext">[Unit]&#x000A;Description=ctdb meta volume&#x000A;Requires=glusterd.service&#x000A;Before=ctdb.service&#x000A;&#x000A;[Mount]&#x000A;What=localhost:meta&#x000A;Where=/mnt/lock&#x000A;Type=glusterfs&#x000A;Options=defaults,_netdev&#x000A;&#x000A;[Install]&#x000A;WantedBy=multi-user.target&#x000A;</code></pre>

<h2 id="installing-the-hosted-engine">Installing the hosted engine</h2>

<p>First we'll start the hosted engine back up on <strong>host one</strong>:</p>

<pre><code class="highlight plaintext"># systemctl start ovirt-ha-agent &amp;&amp; systemctl start ovirt-ha-broker&#x000A;# hosted-engine --set-maintenance --mode=none&#x000A;</code></pre>

<p>Then, wait for a few minutes for the hosted engine to come back up. If you'd like, fire up <code>tail -f /var/log/ovirt-hosted-engine-ha/agent.log</code> to watch its progress. You can get less verbose progress-checking by running <code>hosted-engine --vm-status</code> periodically.</p>

<p>Once the engine is back up and available, head to your <strong>second</strong> machine to configure it as a second host for our oVirt management server:</p>

<pre><code class="highlight plaintext"># screen&#x000A;# hosted-engine --deploy&#x000A;</code></pre>

<p>As with the first machine, the script will ask for the storage type we wish to use. Just as before, answer <code>nfs3</code> and then provide the information for your NFS share. In my case, this is <code>ovirtmount.osas.lab:/engine</code>. </p>

<p>After accessing your storage, the script will detect that there's an existing hosted engine instance, and ask whether you're setting up an additional host. Answer yes, and when the script asks for a Host ID, make it <code>2</code>. The script will then ask for the IP address and root password of your first host, in order to access the rest of the settings it needs. </p>

<p>When the installation process completes, head over to your <strong>third machine</strong> and perform the same steps you did w/ your second host, substituting <code>3</code> for the Host ID.</p>

<p>I found that the installer reset my iptables rules, so on both the second and third hosts, I moved the iptables rules that the installer replaced (but, considerately, backed up) back into place. On my machines, the command looked like this:</p>

<pre><code class="highlight plaintext"># mv /etc/sysconfig/iptables.20141112134450 /etc/sysconfig/iptables&#x000A;# systemctl reload iptables&#x000A;</code></pre>

<h2 id="maintenance-failover-and-storage">Maintenance, failover, and storage</h2>

<p>Once you have everything set up, you should be able to power cycle all three machines and, after a few minutes, have your hosted engine and full oVirt installation back up and running without intervention. </p>

<p>You can bring a single machine down for maintenance by first putting the system into maintenance mode from the oVirt console, and updating, rebooting, shutting down, etc. as desired. </p>

<p>If you bring down two machines at once, you'll run afoul of the Gluster quorum rules that guard us from split-brains states in our storage, and the volumes served by your remaining host will go read-only.</p>

<p>Triple replication is necessary for our hosted engine volume and for the master data volume, but can create additional storage domains that live on just one of your hosts, or distributed across all of them. </p>

<p>Within an oVirt data center, it's easy to migrate VM storage from one data domain to another, so you could save on replication traffic overhead with domains hosted from different Gluster volume types, shuttling disks around as needed when it's time to bring one of your storage hosts down.</p>

<h2 id="till-next-time">Till next time</h2>

<p>If you run into trouble following this walkthrough, I’ll be happy to help you get up and running or get pointed in the right direction. On IRC, I’m jbrooks, ping me in the #ovirt room on OFTC, write a comment below, or give me a shout on Twitter <a href="https://twitter.com/jasonbrooks">@jasonbrooks</a>.</p>

<p>If you’re interested in getting involved with the oVirt Project, you can find all the mailing list, issue tracker, source repository, and wiki information you need <a href="http://www.ovirt.org/Community">here</a>.</p>

</section>
<footer class='post-meta'>
<div class='tags'>
Tags:
<ul class='taglist'></ul>
<li><a href="/blog/tag/centos/">centos</a></li>
<li><a href="/blog/tag/gluster/">gluster</a></li>
<li><a href="/blog/tag/glusterization/">glusterization</a></li>
<li><a href="/blog/tag/ovirt/">ovirt</a></li>
<li><a href="/blog/tag/storage/">storage</a></li>
<li><a href="/blog/tag/virtualization/">virtualization</a></li>
</div>
</footer>
</article>

<section id='blog-comments'></section>
</div>
</section>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<ul class='footer-nav-list'>
<li><a target="_blank" href="http://www.redhat.com/legal/privacy_statement.html">Privacy Policy</a></li>
<li><a target="_blank" href="http://www.redhat.com/legal/legal_statement.html">Terms of Use</a></li>
<li><a target="_blank" href="http://www.redhat.com/about/contact/">Contact Us</a></li>
<li><a target="_blank" href="http://www.redhat.com/about/">About Red Hat</a></li>
</ul>

Copyright &copy; 2012 &ndash; 2013 Red Hat, Inc.
</footer>


<script src="/javascripts/application.js?1430415687" type="text/javascript"></script>
<script>
  (function() {
    \$('article.post').copyright({
      text: '<hr><p>This article originally appeared on <a href="http://community.redhat.com/">community.redhat.com</a>. Follow the community on Twitter at <a href="https://twitter.com/redhatopen">@redhatopen</a>, and find us on <a href="https://www.facebook.com/redhatopen">Facebook</a> and <a href="https://plus.google.com/u/0/b/113258037797946990391/113258037797946990391/posts">Google+</a>.</p> '
    });
  
  }).call(this);
</script>

<!-- eloqua -->
<script src='http://www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script src='http://www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>
<!-- piwik -->
        <script type="text/javascript">
        var _paq = _paq || [];
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
        var u=(("https:" == document.location.protocol) ? "https" : "http") + "://piwik-osasteam.rhcloud.com/piwik/";
        _paq.push(['setTrackerUrl', u+'piwik.php']);
        _paq.push(['setSiteId', 3]);
        var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
        g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
        })();
        </script>
<noscript><p><img src="https://piwik-osasteam.rhcloud.com/piwik/piwik.php?idsite=3" style="border:0;" alt="" /></p></noscript>

</body>
</html>
EOF

%end

%post --nochroot
sed "s/^network.*//g;s/^#virthost02: //g" /run/install/ks.cfg > /mnt/sysimage/root/virthost02-ks.cfg
sed "s/^network.*//g;s/^#virthost03: //g" /run/install/ks.cfg > /mnt/sysimage/root/virthost03-ks.cfg
mkdir -p /mnt/sysimage/home/tmp
grep "^network.*virthost01" /run/install/ks.cfg && dd if=$(df | grep "/run/install" | head -n1 | cut -d" " -f1) of=/mnt/sysimage/home/tmp/CentOS-7-x86_64-Everything-1503-01.iso bs=1M
%end


reboot

